{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "\n",
    "corpusFileName = 'corpusPoems'\n",
    "modelFileName = 'modelLSTM'\n",
    "trainDataFileName = 'trainData'\n",
    "testDataFileName = 'testData'\n",
    "char2idFileName = 'char2id'\n",
    "auth2idFileName = 'auth2id'\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "batchSize = 32\n",
    "char_emb_size = 256\n",
    "\n",
    "hid_size = 256\n",
    "lstm_layers = 3\n",
    "dropout = 0.4\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 0.00027\n",
    "\n",
    "defaultTemperature = 0.4\n",
    "startChar = '{'\n",
    "endChar = '}'\n",
    "unkChar = '@'\n",
    "padChar = '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModelPack(torch.nn.Module):\n",
    "    def preparePaddedBatch(self, source):\n",
    "        device = next(self.parameters()).device\n",
    "        # print(source)\n",
    "        m = max(len(s) for (a,s) in source)\n",
    "        sents = [[self.word2ind.get(w,self.unkTokenIdx) for w in s] for (a,s) in source]\n",
    "        auths = [self.auth2id.get(a,0) for (a,s) in source]\n",
    "        sents_padded = [ s+(m-len(s))*[self.padTokenIdx] for s in sents]\n",
    "        return torch.t(torch.tensor(sents_padded, dtype=torch.long, device=device)), torch.tensor(auths, dtype=torch.long, device=device)\n",
    "    \n",
    "    def save(self,fileName):\n",
    "        torch.save(self.state_dict(), fileName)\n",
    "    \n",
    "    def load(self,fileName,device):\n",
    "        self.load_state_dict(torch.load(fileName,device))\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, auth2id, word2ind, unkToken, padToken, endToken, lstm_layers, dropout):\n",
    "        super(LSTMLanguageModelPack, self).__init__()\n",
    "        #############################################################################\n",
    "        ###  Тук следва да се имплементира инициализацията на обекта\n",
    "        ###  За целта може да копирате съответния метод от програмата за упр. 13\n",
    "        ###  като направите добавки за повече слоеве на РНН, влагане за автора и dropout\n",
    "        #############################################################################\n",
    "        #### Начало на Вашия код.\n",
    "        pass\n",
    "        self.word2ind = word2ind\n",
    "        self.auth2id = auth2id\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.unkTokenIdx = word2ind[unkToken]\n",
    "        self.padTokenIdx = word2ind[padToken]\n",
    "        self.endTokenIdx = word2ind[endToken]\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.lstm = torch.nn.LSTM(embed_size, hidden_size,lstm_layers)\n",
    "        self.embed = torch.nn.Embedding(len(word2ind), embed_size)\n",
    "        self.authembed = torch.nn.Embedding(len(auth2id),hidden_size)\n",
    "        self.projection = torch.nn.Linear(hidden_size,len(word2ind))\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "        #### Край на Вашия код\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, source):\n",
    "        #############################################################################\n",
    "        ###  Тук следва да се имплементира forward метода на обекта\n",
    "        ###  За целта може да копирате съответния метод от програмата за упр. 13\n",
    "        ###  като направите добавка за dropout и началните скрити вектори\n",
    "        #############################################################################\n",
    "        #### Начало на Вашия код.\n",
    "\n",
    " \n",
    "        X,auths = self.preparePaddedBatch(source)\n",
    "        E = self.embed(X[:-1])\n",
    "        h_0 = self.authembed(auths).unsqueeze(0).repeat(self.lstm_layers,1,1)\n",
    "        c_0 = self.authembed(auths).unsqueeze(0).repeat(self.lstm_layers,1,1)\n",
    "        source_lengths = [len(s[1])-1 for s in source]\n",
    "        outputPacked, _ = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(E, source_lengths,enforce_sorted=False),(h_0,c_0))\n",
    "        output,_ = torch.nn.utils.rnn.pad_packed_sequence(outputPacked)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        Z = self.projection(output.flatten(0,1))\n",
    "        # Z = self.projection(self.dropout(output.flatten(0,1)))\n",
    "\n",
    "        Y_bar = X[1:].flatten(0,1)\n",
    "        # Y_bar[Y_bar==self.endTokenIdx] = self.padTokenIdx\n",
    "\n",
    "        H = torch.nn.functional.cross_entropy(Z,Y_bar,ignore_index=self.padTokenIdx)\n",
    "        return H\n",
    "        #### Край на Вашия код\n",
    "        #############################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "corpusSplitString = '@\\n'\n",
    "maxPoemLength = 10000\n",
    "symbolCountThreshold = 100\n",
    "authorCountThreshold = 20\n",
    "\n",
    "def splitSentCorpus(fullSentCorpus, testFraction = 0.1):\n",
    "    random.seed(42)\n",
    "    random.shuffle(fullSentCorpus)\n",
    "    testCount = int(len(fullSentCorpus) * testFraction)\n",
    "    testSentCorpus = fullSentCorpus[:testCount]\n",
    "    trainSentCorpus = fullSentCorpus[testCount:]\n",
    "    return testSentCorpus, trainSentCorpus\n",
    "\n",
    "def getAlphabetAuthors(corpus):\n",
    "    symbols={}\n",
    "    authors={}\n",
    "    for s in corpus:\n",
    "        if len(s) > 0:\n",
    "            n=s.find('\\n')\n",
    "            aut = s[:n]\n",
    "            if aut in authors: authors[aut] += 1\n",
    "            else: authors[aut] = 1\n",
    "            poem = s[n+1:]\n",
    "            for c in poem:\n",
    "                if c in symbols: symbols[c] += 1\n",
    "                else: symbols[c]=1\n",
    "    return symbols, authors\n",
    "\n",
    "def prepareData(corpusFileName, startChar, endChar, unkChar, padChar):\n",
    "    file = open(corpusFileName,'r',encoding=\"utf8\")\n",
    "    poems = file.read().split(corpusSplitString)\n",
    "    symbols, authors = getAlphabetAuthors(poems)\n",
    "    \n",
    "    assert startChar not in symbols and endChar not in symbols and unkChar not in symbols and padChar not in symbols\n",
    "    charset = [startChar,endChar,unkChar,padChar] + [c for c in sorted(symbols) if symbols[c] > symbolCountThreshold]\n",
    "    char2id = { c:i for i,c in enumerate(charset)}\n",
    "    authset = [a for a in sorted(authors) if authors[a] > authorCountThreshold]\n",
    "    auth2id = { a:i for i,a in enumerate(authset)}\n",
    "    \n",
    "    corpus = []\n",
    "    for i,s in enumerate(poems):\n",
    "        if len(s) > 0:\n",
    "            n=s.find('\\n')\n",
    "            aut = s[:n]\n",
    "            poem = s[n+1:]\n",
    "            corpus.append( (aut,[startChar] + [ poem[i] for i in range(min(len(poem),maxPoemLength)) ] + [endChar]) )\n",
    "\n",
    "    testCorpus, trainCorpus  = splitSentCorpus(corpus, testFraction = 0.01)\n",
    "    print('Corpus loading completed.')\n",
    "    return testCorpus, trainCorpus, char2id, auth2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loading completed.\n",
      "Data prepared.\n"
     ]
    }
   ],
   "source": [
    "#prepare\n",
    "testCorpus, trainCorpus, char2id, auth2id =  prepareData(corpusFileName, startChar, endChar, unkChar, padChar)\n",
    "pickle.dump(testCorpus, open(testDataFileName, 'wb'))\n",
    "pickle.dump(trainCorpus, open(trainDataFileName, 'wb'))\n",
    "pickle.dump(char2id, open(char2idFileName, 'wb'))\n",
    "pickle.dump(auth2id, open(auth2idFileName, 'wb'))\n",
    "print('Data prepared.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(trainCorpus, lm, optimizer, epochs, batchSize):\n",
    "    idx = np.arange(len(trainCorpus), dtype='int32')\n",
    "    lm.train()\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(idx)\n",
    "        for b in range(0, len(idx), batchSize):\n",
    "            batch = [ trainCorpus[i] for i in idx[b:min(b+batchSize, len(idx))] ]\n",
    "            H = lm(batch)\n",
    "            optimizer.zero_grad()\n",
    "            H.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch:\",epoch+1,'/',epochs,\", Batch:\",b // batchSize, '/', len(idx) // batchSize, \", loss: \",H.item())\n",
    "\n",
    "def perplexity(lm, testCorpus, batchSize):\n",
    "    lm.eval()\n",
    "    H = 0.\n",
    "    c = 0\n",
    "    for b in range(0,len(testCorpus),batchSize):\n",
    "        batch = testCorpus[b:min(b+batchSize, len(testCorpus))]\n",
    "        l = sum(len(s)-1 for s in batch)\n",
    "        c += l\n",
    "        with torch.no_grad():\n",
    "            H += l * lm(batch)\n",
    "    return math.exp(H/c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = pickle.load(open(testDataFileName, 'rb'))\n",
    "trainCorpus = pickle.load(open(trainDataFileName, 'rb'))\n",
    "char2id = pickle.load(open(char2idFileName, 'rb'))\n",
    "auth2id = pickle.load(open(auth2idFileName, 'rb'))\n",
    "\n",
    "\n",
    "lm = LSTMLanguageModelPack(char_emb_size, hid_size, auth2id, char2id, unkChar, padChar, endChar, lstm_layers=lstm_layers, dropout=dropout).to(device)\n",
    "\n",
    "# to continue training existing model\n",
    "# lm.load(modelFileName,device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lm.parameters(), lr=learning_rate)\n",
    "trainModel(trainCorpus, lm, optimizer, epochs, batchSize)\n",
    "lm.save('/content/drive/MyDrive/neural poet/modelLSTM5')\n",
    "\n",
    "print('Model perplexity: ',perplexity(lm, testCorpus, batchSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateText(model, char2id, auth, startSentence, limit=1000, temperature=1.):\n",
    "    # model е инстанция на обучен LSTMLanguageModelPack обект\n",
    "    # char2id е речник за символите, връщащ съответните индекси\n",
    "    # startSentence е началния низ стартиращ със символа за начало '{'\n",
    "    # limit е горна граница за дължината на поемата\n",
    "    # temperature е температурата за промяна на разпределението за следващ символ\n",
    "    \n",
    "    result = startSentence[1:]\n",
    "    id2char = dict(enumerate(char2id))\n",
    "    #############################################################################\n",
    "    ###  Тук следва да се имплементира генерацията на текста\n",
    "    #############################################################################\n",
    "    #### Начало на Вашия код.\n",
    "    def get_next(model,current,auth,hidden=None):\n",
    "      source = [(auth,s) for s in current]\n",
    "      # print(source)\n",
    "      X,auth = model.preparePaddedBatch(source)\n",
    "      E = model.embed(X)\n",
    "      source_lengths = [len(s) for s in current]\n",
    "      auth = model.authembed(auth)\n",
    "\n",
    "      if hidden == None:\n",
    "        hidden = (auth.unsqueeze(0).repeat(model.lstm_layers,1,1),auth.unsqueeze(0).repeat(model.lstm_layers,1,1))\n",
    "      \n",
    "      output_packed,hidden = model.lstm(torch.nn.utils.rnn.pack_padded_sequence(E,source_lengths,enforce_sorted=False),hidden)\n",
    "      output,_ = torch.nn.utils.rnn.pad_packed_sequence(output_packed)\n",
    "\n",
    "      output = model.dropout(output)\n",
    "      Z = model.projection(output.flatten(0,1))\n",
    "\n",
    "      ind = len(current)-1\n",
    "      p = torch.nn.functional.softmax(torch.div(Z, temperature), dim=1).data\n",
    "      p, top_char_ind = p.topk(32)\n",
    "      top_char_ind = top_char_ind.cpu().numpy().squeeze()\n",
    "      p = p[ind].cpu().numpy().squeeze()\n",
    "      \n",
    "      if type(top_char_ind[ind]) is np.ndarray:\n",
    "        t = np.random.choice(top_char_ind[ind], p=p / np.sum(p))\n",
    "      else :\n",
    "        t = np.random.choice(top_char_ind, p=p / np.sum(p))\n",
    "      # print(t)\n",
    "      # print(id2char[t])\n",
    "      return id2char[t],hidden\n",
    "    \n",
    "    source = startSentence ### maybe [x for x in result]\n",
    "    # print([x for x in result])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    source = [x for x in source]\n",
    "    size = len(source)\n",
    "    output, h = get_next(model,source,auth)\n",
    "    source.append(output)\n",
    "\n",
    "    while output !=endChar and size <= limit:\n",
    "      output, h = get_next(model,source[size],auth,h)\n",
    "      source.append(output)\n",
    "      size += 1\n",
    "    \n",
    "    result = ''.join(source)\n",
    "    pass\n",
    "    \n",
    "    #### Край на Вашия код\n",
    "    #############################################################################\n",
    "\n",
    "    return result[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = pickle.load(open(char2idFileName, 'rb'))\n",
    "auth2id = pickle.load(open(auth2idFileName, 'rb'))\n",
    "lm = LSTMLanguageModelPack(char_emb_size, hid_size, auth2id, char2id, unkChar, padChar, endChar, lstm_layers=lstm_layers, dropout=dropout).to(device)\n",
    "lm.load(modelFileName,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На мен\n",
      "От черния призрак на деня\n",
      "в тъмнината с небе се спряха,\n",
      "пред теб на сълзи вече неми\n",
      "и с него се всичко в него дивно\n",
      "с нас от своя стар да се настане,\n",
      "с тебе да спомня, че поне вече\n",
      "с теб да ни подреди и да пея\n",
      "и да бъде последен и скрит,\n",
      "този път на мойта загадъчна\n",
      "на душата ми на нов свят нетраен,\n",
      "и ти вече не ще се върна в твойта\n",
      "с теб погледа ти и устни тук —\n",
      "гигантски стон и трева и сърцето\n",
      "за покрива при света не се спрял,\n",
      "ни тръгвам за мене сърцето си страдание\n",
      "и в моя миг посред дълбоко сърце,\n",
      "че съм непостоян, в който се връща\n",
      "с венчана с глава и подкрепен въздух.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generateText(lm, char2id, \"Иван Вазов\", \"{\", limit=1000, temperature=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c355075b6998facf7f6fa9e896a1ead8e80651e7d7c1eb199c4a8b8bb065b0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
